{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2\n",
    "\n",
    "**Map-Reduce** deals with failures and slow tasks by re-launching the tasks on other machines. This functionality is enabled by the requirement that individual tasks in a Map Reduce job are \n",
    "\n",
    "- **idempotent** and \n",
    "- have **no side effect**\n",
    "\n",
    "Using memory instead of disks offers two huge benefits. \n",
    "\n",
    "- memory is much faster than disks\n",
    "- Keeping intermediate results in memory means that they do not have to be converted into a format that can be stored on disks. The converting process requires serialization and de-serialization which are very expensive tasks.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Spark powers a stack of high-level these tools.\n",
    "\n",
    "![](https://spark.apache.org/images/spark-stack.png)\n",
    "(Ref - https://spark.apache.org/)\n",
    "\n",
    "![](https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/History.png)\n",
    "(Ref - https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/)\n",
    "\n",
    "## Spark\n",
    "\n",
    "[Ref - Apache Spark 101](http://www.slideshare.net/paulszulc/apache-spark-101-in-50-min)\n",
    "\n",
    "[Ref - spark.apache.org](https://spark.apache.org/docs/1.3.0/cluster-overview.html)\n",
    "\n",
    "![](https://spark.apache.org/docs/1.3.0/img/cluster-overview.png)\n",
    "\n",
    "### Spark Context\n",
    "\n",
    "- tells Spark how and where to access a cluster\n",
    "\n",
    "### RDDs, Resilient Distributed Datasets\n",
    "\n",
    "RDDs\n",
    "\n",
    "- are **immutable**, in other words they can not be changed after constructed\n",
    "- can be created by transformations applied to existing RDDs\n",
    "- enable parallel operations on collections of distributed data\n",
    "- track lineage information to enable efficient recomputation of lost data\n",
    "\n",
    "All transformations in Spark are **lazy**, in that they do no compute their results right away. instead, they just remember the transformations applied to some base dataset. The transformations are only compueted when an action requires a result to be returned to the driver program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Createing RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[91] at parallelize at PythonRDD.scala:392"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md MapPartitionsRDD[93] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distFile = sc.textFile(\"README.md\", 4)\n",
    "distFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "- `map`\n",
    "- `filter`\n",
    "- `distinct`\n",
    "- `flatMap`\n",
    "\n",
    "function literals (lambda) are clusures automatically passed to workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "rdd.map(lambda x: x * 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([1, 4, 2, 2, 3])\n",
    "rdd2.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 2, 2, 3]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect() # immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6], [2, 7], [3, 8], [4, 9]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.distinct().map(lambda x: [x, x + 5]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1, 2, -2, 3, -3]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([1, 2, 3])\n",
    "rdd3.flatMap(lambda x: [x, -x]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Actions\n",
    "\n",
    "- cause Spark execute recipe to transform source\n",
    "- is the primary mechanism for getting results out of Spark\n",
    "- are not lazily evaluated\n",
    "\n",
    "- `reduce`\n",
    "- `take`\n",
    "- `collect`\n",
    "- `takeOrdered`\n",
    "\n",
    "**nothing happens when we acually do that until we execute an action**\n",
    "\n",
    "- Actions cause parallel computation to be **immediately executed**\n",
    "- Transformations **lazily create** new RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.reduce(lambda a, b: a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([5, 3, 1, 2])\n",
    "rdd.takeOrdered(3, lambda s: -1 * s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Spark Programming Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching RDDs\n",
    "\n",
    "```python\n",
    "lines = sc.textFile(\"...\", 4)\n",
    "comments = lines.filter(isComment)\n",
    "print lines.count(), comments.count() # comments.count will recompute `lines` to prevent this, we can use `cache`\n",
    "```\n",
    "\n",
    "```python\n",
    "lines = sc.textFile(\"...\", 4)\n",
    "lines.cache\n",
    "\n",
    "comments = lines.filter(isComment)\n",
    "print lines.count(), comments.count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Program Life-cycle\n",
    "\n",
    "1. Create RDDs from external data or **paralleize** a collection in your driver program\n",
    "\n",
    "2. Lazily **transform** them into new RDDs\n",
    "\n",
    "3. `cache()` some RDDs for reuse\n",
    "\n",
    "4. perform **actions** to execute parallel computation and produce result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Key-Value RDDs\n",
    "\n",
    "- `reduceByKey`\n",
    "- `sortByKey`\n",
    "- `groupByKey`\n",
    "\n",
    "Be careful using `groupByKey()` as it can cause a lot of data movement across the network and create large iterables at workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "rdd.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (1, 'b'), (2, 'c')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([(1, 'a'), (2, 'c'), (1, 'b')])\n",
    "rdd2.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pySpark Closures\n",
    "\n",
    "- one closure per work\n",
    "- sent for every task\n",
    "- changes to global variables at workers are not sent to the driver\n",
    "\n",
    "The problem is, \n",
    "\n",
    "- closures are (re-)sent with **every** job\n",
    "- inefficient to send large data to each worker\n",
    "- closures are one way (driver -> worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "\n",
    "- efficiently send large, **read-only** value to all workers instead of to each task\n",
    "- usually distributed using efficient broadcast algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast Example\n",
    "\n",
    "```python\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "  country = lookupCountry(sign_count[0], signPrefixes)\n",
    "  count = sign_count[1]\n",
    "  return (country, count)\n",
    "  \n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(processSignCount)\n",
    "                        .reduceByKey(lambda x, y: x + y))\n",
    "```\n",
    "\n",
    "We can improve above example using **broadcast**\n",
    "\n",
    "```python\n",
    "signPrefixes = broadcast(loadCallSignTable())\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "  country = lookupCountry(sign_count[0], signPrefixes.value)\n",
    "  count = sign_count[1]\n",
    "  return (country, count)\n",
    "  \n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(processSignCount)\n",
    "                        .reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulator\n",
    "\n",
    "- aggregate values from workers back to driver\n",
    "- only driver can access value of accumulator\n",
    "- for tasks, accumulators are **write-only**\n",
    "- \n",
    "\n",
    "**accumulators can be used in actions or transformations**, but transformations: **no guarantees**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "def f(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    \n",
    "rdd.foreach(f)\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulator Example\n",
    "\n",
    "```python\n",
    "  file = sc.textFile(input)\n",
    "  blankLines = sc.accumulator(0)\n",
    "    \n",
    "def extractCallSigns(line):\n",
    "  global blankLines\n",
    "  \n",
    "  if (line == \"\"):\n",
    "    blankLines += 1\n",
    "  \n",
    "  return line.split(\" \")\n",
    "  \n",
    "callSigns = file.flatMap(extractCallSigns)\n",
    "\n",
    "print \"Blank lines: %d\" % blankLines.value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
